{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# for teacher\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Side variables\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_type = np.float32\n",
    "torch_type = torch.float32\n",
    "batch_size = 64\n",
    "window_length = 1024 # Data points will hold 1024 tokens of observations (should be 1/4 average song length in tokens, remember to remove outliers)\n",
    "window_step_size = 32 # Sliding window will move this much each time (higher numbers means less data, but less overfitting to similar data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] # <-- put data here\n",
    "\n",
    "# Assume already tokenized (this is for transformer, must be adapted for other models)\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, window_length, step_size):\n",
    "        self.temp_data = data\n",
    "        self.window_length = window_length\n",
    "        self.step_size = step_size\n",
    "        self.final_data = self.apply_window()\n",
    "\n",
    "    def apply_window(self):\n",
    "        # return sliding window data + labels\n",
    "        train_examples = []\n",
    "        # cycle through each window configuration, calculating start index and end index\n",
    "        for start_idx in range(0, len(self.temp_data) - self.window_length + 1, self.step_size):\n",
    "            end_idx = start_idx + self.window_length\n",
    "            train_example = self.temp_data[start_idx:end_idx] # training of length window_length\n",
    "            train_examples.append(train_example)\n",
    "            \n",
    "        return train_examples\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.final_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = self.final_data[idx]\n",
    "\n",
    "        return torch.tensor(window).to(device)\n",
    "    \n",
    "\n",
    "dataset = MusicDataset(data, window_length, window_step_size)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "training_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Variables\n",
    "seq_len = 256\n",
    "embed_size = 512 # larger embed size may require larger dropout\n",
    "dropout = 0.2\n",
    "lr = 1e-4\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Music_RNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, dropout):\n",
    "        super(Music_RNN, self).__init__()\n",
    "        self.RNN = nn.RNN(input_size, embed_size, batch_first=True, dropout=dropout)\n",
    "        self.ff = nn.Linear(embed_size, 1) # predicting next input autoregressively\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should be (batch_size, seq_len)\n",
    "        x = self.RNN(x)\n",
    "        x_pred = self.ff(x)\n",
    "\n",
    "        return x_pred # (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = Music_RNN(seq_len, embed_size, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt loss and optimizer as needed\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model_rnn.parameters(), lr=lr)\n",
    "\n",
    "# train\n",
    "for epoch in range(epochs):\n",
    "    # Set to train\n",
    "    model_rnn.train()\n",
    "    # keep cumalitive losses\n",
    "    total_losses = 0.0\n",
    "\n",
    "    for batch in enumerate(training_dataloader):\n",
    "        inputs, targets = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model_rnn.forward(inputs) # make sure dimensions line up\n",
    "\n",
    "        loss = criterion(preds, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_losses += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {total_losses / len(training_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "model_rnn.eval()\n",
    "\n",
    "total_losses = 0.0\n",
    "\n",
    "# validation\n",
    "with torch.no_grad():\n",
    "    for batch in validation_dataloader:\n",
    "        inputs, targets = batch\n",
    "\n",
    "        preds = model_rnn.forward(inputs) # make sure dimensions line up\n",
    "\n",
    "        loss = criterion(preds, targets)\n",
    "        total_losses += loss.item()\n",
    "\n",
    "    print(f\"Loss: {total_losses / len(validation_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAYYYYYYYYY Transformer Time WOOOOOOOOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Model variables\n",
    "d_model = 128 # Embed Dim\n",
    "n_encoder_layers = 6 # lower if not that complex and wanting speedup\n",
    "n_decoder_layers = n_encoder_layers\n",
    "n_heads = 8 # Number of Attention Heads\n",
    "d_ff = 512 # Feed Forward Dimensionality (AIAYN paper reccomends 4 times d_model size)\n",
    "learning_rate = 1e-5 # Maybe increase?\n",
    "num_epochs = 100 # Change this later so it doesn't take 10 years to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard positonal encoding used here, could also try time encoding since notes have different timestamps\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, win_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(win_len, d_model) # (win_len, d_model)\n",
    "        position = torch.arange(0, win_len, dtype=torch_type).unsqueeze(1) # (win_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-torch.log(torch.tensor(10000.0))/d_model)) # (d_model/2)\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        pe = pe.unsqueeze(0) # (1, win_len, d_model)\n",
    "        self.pe = pe.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "# mask\n",
    "def generate_causal_mask(size):\n",
    "    # size should be target size\n",
    "    mask = torch.triu(torch.ones(size, size, device=device), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    '''\n",
    "    [0, -inf, -inf, -inf]\n",
    "    [0,   0,  -inf, -inf]\n",
    "    [0,   0,    0,  -inf]\n",
    "    [0,   0,    0,    0 ]\n",
    "    Yay for triangle masking\n",
    "    '''\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, num_tokens, d_model=d_model, nhead=n_heads, dim_ff=d_ff, win_len=window_length, layers=n_encoder_layers):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.dim_ff = dim_ff\n",
    "        self.win_len = win_len\n",
    "        self.layers = layers\n",
    "\n",
    "        # pre-transformer\n",
    "        self.tok_embed = nn.Embedding(num_tokens, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, win_len)\n",
    "\n",
    "        # transformer part\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass sequential data through embedding and apply positional encoding\n",
    "        # x is (batch_size, win_len)\n",
    "        x_seq = self.tok_embed(x) # (batch_size, win_len, d_model)\n",
    "        x_seq = self.pos_enc(x_seq) # (batch_size, win_len, d_model)\n",
    "        # now through transformer encoder layers\n",
    "        encoder_output = self.encoder(x_seq)\n",
    "\n",
    "        return encoder_output\n",
    "    \n",
    "\n",
    "\n",
    "# Actual decoder\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, num_tokens, d_model=d_model, nhead=n_heads, dim_ff=d_ff, win_len=window_length, layers=n_encoder_layers):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nheads = nhead\n",
    "        self.dim_ff = dim_ff\n",
    "        self.win_len = win_len\n",
    "        self.layers = layers\n",
    "\n",
    "        # pre-transformer\n",
    "        self.tok_embed = nn.Embedding(num_tokens, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, win_len)\n",
    "        self.out_proj = nn.Linear(d_model, num_tokens) # final projection for token prediction\n",
    "\n",
    "        # transformer part\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_ff, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=self.layers)\n",
    "\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None):\n",
    "        '''\n",
    "        Memory is encoder output\n",
    "        Tgt is the real note  # (batch_size, 1)\n",
    "        Tgt_mask masks the predictions, but won't be needed unless we decide to predict multiple steps at once\n",
    "        '''\n",
    "\n",
    "        x_seq = self.tok_embed(tgt) # (batch_size, 1, d_model)\n",
    "        x_seq = self.pos_enc(x_seq) # (batch_size, 1, d_model)\n",
    "\n",
    "        decoder_output = self.decoder(x_seq, memory, tgt_mask=tgt_mask) # (batch_size, 1, d_model)\n",
    "        pred = self.out_proj(decoder_output) # (batch)\n",
    "      \n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, num_tokens, d_model, nhead, dim_ff, win_len, layers):\n",
    "        super(TransformerAutoencoder, self).__init__()\n",
    "        self.encoder = EncoderLayer(num_tokens, d_model, nhead, dim_ff, win_len, layers)\n",
    "        self.decoder = DecoderLayer(num_tokens, d_model, nhead, dim_ff, win_len, layers, out_days)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        # tgt is (batch_size, win_len)\n",
    "        memory = self.encoder(tgt)\n",
    "        my_mask = generate_causal_mask(tgt.size(1)) # win_len by win_len mask\n",
    "        reconstructed = self.decoder(tgt, memory, tgt_mask=my_mask)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "num_tokens = len(set(list(data)))\n",
    "model = TransformerAutoencoder(num_tokens, d_model, n_heads, d_ff, window_length, n_encoder_layers, days_to_predict).to(device)\n",
    "\n",
    "# Use Adam cause he's so cool\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0 # cumalative loss\n",
    "\n",
    "    # Cycle through each batch\n",
    "    for batch in training_dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Pass through model\n",
    "        output = model(batch)\n",
    "        # Determine loss\n",
    "        loss = criterion(batch, output)\n",
    "        # Update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Add to epoch_loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Now show average loss for epoch\n",
    "    avg_epoch_loss = epoch_loss / len(training_dataloader)\n",
    "    print(f\"Epoch: [{epoch+1}/{num_epochs}]   Epoch Average Loss: {avg_epoch_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above doesn't actually autoregressively select, I will add that later. Now it just recreates sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
