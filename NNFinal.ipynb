{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# for teacher\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Side variables\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_type = np.float32\n",
    "torch_type = torch.float32\n",
    "batch_size = 64\n",
    "window_length = 1024 # Data points will hold 1024 tokens of observations (should be 1/4 average song length in tokens, remember to remove outliers)\n",
    "window_step_size = 32 # Sliding window will move this much each time (higher numbers means less data, but less overfitting to similar data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data / Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<simul>', 'rest_1/2', 'noteD3_1/2', '</simul>', '<simul>']\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "data = []\n",
    "with open(\"./out.txt\", \"r\") as file:\n",
    "    data = file.read().splitlines()\n",
    "\n",
    "print(data[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4195\n"
     ]
    }
   ],
   "source": [
    "# now tokenize\n",
    "unique_tokens = set(list(data) + [\"PAD\"])\n",
    "print(len(unique_tokens))\n",
    "# mapping\n",
    "note_to_token = {note: idx for idx, note in enumerate(unique_tokens)}\n",
    "token_to_note = {idx: note for note, idx in note_to_token.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1749\n",
      "711\n"
     ]
    }
   ],
   "source": [
    "end_token = note_to_token[\"<end_song>\"]\n",
    "pad_token = note_to_token[\"PAD\"]\n",
    "print(end_token)\n",
    "print(pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inp = [note_to_token[tok] for tok in data]\n",
    "\n",
    "\n",
    "# Assume already tokenized (this is for transformer, must be adapted for other models)\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, window_length, step_size):\n",
    "        self.temp_data = data\n",
    "        self.window_length = window_length\n",
    "        self.step_size = step_size\n",
    "        self.final_data = self.apply_window()\n",
    "\n",
    "    def apply_window(self):\n",
    "        train_examples = []\n",
    "        song_data = []\n",
    "        cur_seq = []\n",
    "\n",
    "        # first split data by songs\n",
    "        for tok in self.temp_data:\n",
    "            cur_seq.append(tok)\n",
    "            if(tok == end_token):\n",
    "                song_data.append(cur_seq)\n",
    "                cur_seq = []\n",
    "        \n",
    "        # then apply sliding windows\n",
    "        for song in song_data:\n",
    "            idx = 0\n",
    "            # create sliding windows\n",
    "            for start_idx in range(0, len(song) - self.window_length + 1, self.step_size):\n",
    "                end_idx = start_idx + self.window_length\n",
    "                idx = end_idx # store most recent end_idx\n",
    "                train_example = song[start_idx:end_idx] # training of length window_length\n",
    "                train_examples.append(train_example)\n",
    "            \n",
    "            # now add ending and pad (so model can learn how to finish predicting)\n",
    "            train_example = song[idx:]\n",
    "            train_example += [0] * (self.window_length - len(train_example))\n",
    "            train_examples.append(train_example)\n",
    "\n",
    "\n",
    "        return train_examples\n",
    "\n",
    "    #def apply_window(self):\n",
    "    #    # return sliding window data + labels\n",
    "    #    train_examples = []\n",
    "    #    # cycle through each window configuration, calculating start index and end index\n",
    "    #    for start_idx in range(0, len(self.temp_data) - self.window_length + 1, self.step_size):\n",
    "    #        end_idx = start_idx + self.window_length\n",
    "    #        train_example = self.temp_data[start_idx:end_idx] # training of length window_length\n",
    "    #        train_examples.append(train_example)\n",
    "    #        \n",
    "    #    return train_examples\n",
    "    \n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.final_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = self.final_data[idx]\n",
    "\n",
    "        return torch.tensor(window).to(device)\n",
    "    \n",
    "\n",
    "dataset = MusicDataset(data_inp, window_length, window_step_size)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "training_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "vocab_len = len(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Variables\n",
    "\n",
    "seq_len = 256\n",
    "embed_size = 512 # larger embed size may require larger dropout\n",
    "dropout = 0.2\n",
    "lr = 1e-4\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Music_RNN(nn.Module):\n",
    "    def __init__(self, vocab_len, input_size, hidden_size, dropout):\n",
    "        super(Music_RNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_len, input_size)\n",
    "        self.RNN = nn.RNN(input_size, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.ff = nn.Linear(hidden_size, vocab_len) # predicting next input autoregressively\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should be (batch_size, seq_len)\n",
    "        x = self.embed(x) # (batch_size, seq_len, input_size)\n",
    "        x, _ = self.RNN(x) # (batch_size, seq_len, hidden_size)\n",
    "        x_pred = self.ff(x) # batch_size, seq_len, vocab_len)\n",
    "\n",
    "        return x_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsu/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_rnn = Music_RNN(vocab_len, seq_len, embed_size, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 4.63392983511655\n"
     ]
    }
   ],
   "source": [
    "# adapt loss and optimizer as needed\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model_rnn.parameters(), lr=lr)\n",
    "\n",
    "# train\n",
    "for epoch in range(epochs):\n",
    "    # Set to train\n",
    "    model_rnn.train()\n",
    "    # keep cumalitive losses\n",
    "    total_losses = 0.0\n",
    "\n",
    "    for batch in training_dataloader:\n",
    "        inp = batch[:, :-1] # grabs all but last\n",
    "        tgt = batch[:, 1:] # grabs all except first (shifted once)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model_rnn.forward(inp) # make sure dimensions line up\n",
    "\n",
    "        loss = criterion(preds.view(-1, vocab_len), tgt.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_losses += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {total_losses / len(training_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.098278461500656\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "model_rnn.eval()\n",
    "\n",
    "total_losses = 0.0\n",
    "\n",
    "# validation\n",
    "with torch.no_grad():\n",
    "    for batch in validation_dataloader:\n",
    "        inp = batch[:, :-1] # grabs all but last\n",
    "        tgt = batch[:, 1:] # grabs all except first (shifted once)\n",
    "\n",
    "        preds = model_rnn.forward(inp) # make sure dimensions line up\n",
    "\n",
    "        loss = criterion(preds.view(-1, vocab_len), tgt.reshape(-1))\n",
    "        total_losses += loss.item()\n",
    "\n",
    "    print(f\"Loss: {total_losses / len(validation_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAYYYYYYYYY Transformer Time WOOOOOOOOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Model variables\n",
    "d_model = 128 # Embed Dim\n",
    "n_decoder_layers = 4 # lower if not that complex and wanting speedup\n",
    "n_heads = 4 # Number of Attention Heads\n",
    "d_ff = 512 # Feed Forward Dimensionality (AIAYN paper reccomends 4 times d_model size)\n",
    "learning_rate = 1e-5 # Maybe increase?\n",
    "num_epochs = 1 # Change this later so it doesn't take 10 years to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard positonal encoding used here, could also try time encoding since notes have different timestamps\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, win_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(win_len, d_model) # (win_len, d_model)\n",
    "        position = torch.arange(0, win_len, dtype=torch_type).unsqueeze(1) # (win_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-torch.log(torch.tensor(10000.0))/d_model)) # (d_model/2)\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        pe = pe.unsqueeze(0) # (1, win_len, d_model)\n",
    "        self.pe = pe.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "# mask\n",
    "def generate_causal_mask(size):\n",
    "    # size should be target size\n",
    "    mask = torch.triu(torch.ones(size, size, device=device), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    '''\n",
    "    [0, -inf, -inf, -inf]\n",
    "    [0,   0,  -inf, -inf]\n",
    "    [0,   0,    0,  -inf]\n",
    "    [0,   0,    0,    0 ]\n",
    "    Yay for triangle masking\n",
    "    '''\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual decoder\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens, d_model=d_model, nhead=n_heads, dim_ff=d_ff, win_len=window_length, layers=n_decoder_layers):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nheads = nhead\n",
    "        self.dim_ff = dim_ff\n",
    "        self.win_len = win_len\n",
    "        self.layers = layers\n",
    "\n",
    "        # pre-transformer\n",
    "        self.tok_embed = nn.Embedding(num_tokens, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, win_len)\n",
    "        self.out_proj = nn.Linear(d_model, num_tokens) # final projection for token prediction\n",
    "\n",
    "        # transformer part\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_ff, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=self.layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, tgt_mask=None):\n",
    "        '''\n",
    "        x is the tokenized notes  # (batch_size, win_len)\n",
    "        tgt_mask masks the predictions\n",
    "        '''\n",
    "\n",
    "        x_seq = self.tok_embed(x) # (batch_size, 1, d_model)\n",
    "        x_seq = self.pos_enc(x_seq) # (batch_size, 1, d_model)\n",
    "\n",
    "        decoder_output = self.decoder(x_seq, x_seq, tgt_mask=tgt_mask) # (batch_size, win_len, d_model)\n",
    "        pred = self.out_proj(decoder_output) # (batch, win_len, num_tokens) \n",
    "      \n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1]   Epoch Average Loss: 7.350050236671381\n"
     ]
    }
   ],
   "source": [
    "my_mask = generate_causal_mask(window_length) # win_len by win_len mask\n",
    "\n",
    "# Create model\n",
    "num_tokens = vocab_len\n",
    "model = MusicTransformer(num_tokens, d_model, n_heads, d_ff, window_length, n_decoder_layers).to(device)\n",
    "\n",
    "# Use Adam cause he's so cool\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0 # cumalative loss\n",
    "\n",
    "    # Cycle through each batch\n",
    "    for batch in training_dataloader:\n",
    "        inp = batch[:, :-1] # grabs all but last\n",
    "        tgt = batch[:, 1:] # grabs all except first (shifted once)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Pass through model\n",
    "        output = model(inp)\n",
    "        # Determine loss\n",
    "        loss = criterion(output.view(-1, num_tokens), tgt.reshape(-1))\n",
    "        # Update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Add to epoch_loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Now show average loss for epoch\n",
    "    avg_epoch_loss = epoch_loss / len(training_dataloader)\n",
    "    print(f\"Epoch: [{epoch+1}/{num_epochs}]   Epoch Average Loss: {avg_epoch_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.4589960852334665\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0 # cumalative loss\n",
    "\n",
    "# Cycle through each batch\n",
    "for batch in validation_dataloader:\n",
    "    inp = batch[:, :-1] # grabs all but last\n",
    "    tgt = batch[:, 1:] # grabs all except first (shifted once)\n",
    "\n",
    "    # Pass through model\n",
    "    output = model(inp)\n",
    "    # Determine loss\n",
    "    loss = criterion(output.view(-1, num_tokens), tgt.reshape(-1))\n",
    "    epoch_loss += loss.item()\n",
    "    \n",
    "# Now show average loss for \n",
    "avg_epoch_loss = epoch_loss / len(validation_dataloader)\n",
    "print(f\"Validation Loss: {avg_epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate music\n",
    "# for now, generate a fixed length, if we add start and end tokens then we can also do that\n",
    "def generate_music(model, start_tokens, temperature=1.0, num_to_generate=window_length):\n",
    "    model.eval()\n",
    "\n",
    "    predicted_tokens = torch.tensor(start_tokens).clone().detach().unsqueeze(0) # (1, len(start_tokens))\n",
    "    # iteratively predict then add to start_tokens\n",
    "    for i in range(num_to_generate):\n",
    "        with torch.no_grad():\n",
    "            # grab most recent tokens\n",
    "            logits = model(predicted_tokens[:, -(window_length):]) # (1, len(predicted_tokens), num_tokens)\n",
    "            logits = logits[:, -1, :] # grabs last token \n",
    "            probs = torch.softmax(logits / temperature, dim=-1) # (1, num_tokens)\n",
    "            pred_token = torch.multinomial(probs, num_samples=1) # (1, 1)\n",
    "\n",
    "        predicted_tokens = torch.cat([predicted_tokens, pred_token], dim=1) # (1, len(current sequence length))\n",
    "\n",
    "    return predicted_tokens.squeeze(0).tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_818374/2949307198.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predicted_tokens = torch.tensor(start_tokens).clone().detach().unsqueeze(0) # (1, len(start_tokens))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noteF5_1/4', 'noteC4_1/2', 'noteC4_1/4', '</simul>', 'noteF2_341/512', 'noteA6_1/3', 'noteE-3_1/10', 'noteF#5_17/256', 'noteF5_7/120', 'noteF6_1', 'noteB1_1/4', 'noteE4_3/4', 'noteB4_2/5', 'noteB-4_2', 'noteC#4_17/480', 'rest_67/240', 'noteC4_51/1024', 'noteF5_2', 'noteF#2_2', 'noteG#3_1/24', 'noteE-6_3', 'noteE-2_1/4', 'noteE5_1/15', 'noteC#7_1/5', 'noteE3_683/1024', 'noteD2_1/2', 'noteF5_1/2', 'noteE-7_3/2', 'noteC3_37/240', 'noteC#1_3/5', 'noteD3_113/1024', 'noteD4_1/20', 'noteB-2_195/1024', 'noteE-6_3/2', 'noteF#3_23/160', 'noteB-3_17/96', 'noteC2_1/4', 'noteE-3_1/15', 'noteG3_2/5', 'noteF1_1/4', 'noteB-6_107/480', 'noteB3_6', 'rest_65/16', 'noteE7_1/3', 'noteC#3_1', 'noteC#4_3/8', 'rest_65/16', 'noteF6_2/5', 'noteE2_85/512', 'noteF3_1/16', 'noteG6_23/160', 'noteE-3_37/240', 'noteE-5_69/1024', 'noteD6_2', 'noteF#2_2/3', 'noteB2_6', 'noteA5_13/96', 'noteG3_7/8', 'noteD3_4', 'noteC5_11/96', 'noteB5_1/16', 'noteG3_1/4', 'noteB3_341/1024', 'noteD1_41/512', 'noteE4_85/512', 'noteG#4_1/15', 'noteC3_79/1024', '</simul>', 'noteG5_6', 'noteF#2_1/3', 'noteE6_1/3', 'noteC6_615/1024', 'noteD6_2/3', 'noteB-5_171/1024', '</simul>', 'noteE-6_3/20', 'noteF#2_23/160', 'noteB-3_1/6', 'noteA3_1/16', 'noteF2_53/480', '</simul>', 'noteF3_171/512', 'noteF5_1/10', 'noteC#4_3/28', 'noteG5_3/4', 'noteB3_79/1024', 'noteC#7_73/1024', 'noteA6_13/256', 'noteC6_683/1024', 'noteE4_1/12', '</simul>', 'noteG4_1/4', 'noteG2_1/12', 'noteG3_39/512', '</simul>', 'noteA6_1/3', 'noteG3_6', '</simul>', 'noteE2_195/1024', 'noteF#3_69/1024', 'noteE-2_1', 'noteG5_7/4', 'noteF7_65/1024', 'noteA5_29/240', 'noteB-2_85/512', 'noteE4_39/512', 'noteA5_1/8', 'noteC#2_3/4', 'noteB5_3/16', 'noteC4_29/240', 'noteE-3_1/3', 'noteE7_1/2', 'noteA4_17/480', 'noteD3_3/5', 'noteF#3_7/8', 'noteB-4_1', 'noteC3_17/160', 'noteB-4_11/120', 'noteC3_39/512', 'noteE1_3', 'noteC#2_1/24', 'noteA1_1/2', 'noteF4_1/24', 'noteD2_341/1024', 'noteE-4_1', 'noteF5_1/12', '</simul>', 'noteC#2_1', 'noteA5_7/8', 'noteE-5_3/2', 'noteE6_3', 'noteG#2_41/512', 'noteB-2_1/4', 'noteG#5_2', 'rest_1/32', 'noteE-5_1/4', 'noteC#6_37/240', 'rest_1', 'noteC#5_3/8', 'noteE2_1/2', 'noteA1_1/20', 'noteE-6_171/512', 'noteE3_1', 'noteG3_23/480', 'noteF#7_1/10', 'noteC5_151/1024', 'noteB3_53/480', 'noteE-1_1', 'noteD3_4', '</simul>', 'noteA5_6/7', 'noteF5_2', 'noteC2_17/160', 'noteA6_13/256', 'noteA5_1/15', 'noteG#1_1/4', 'noteE3_3/4', 'noteC#4_11/240', 'noteC#7_1/8', 'noteG#3_1/4', 'noteG3_1', 'noteB-5_3/4', 'noteE3_85/1024', 'rest_1/64', 'noteB3_1/5', 'noteC6_57/1024', 'noteE-3_1/10', 'noteB-4_113/1024', 'noteB-5_0', 'noteB-3_113/1024', 'noteC3_3/4', 'noteB-1_3/8', 'noteB-3_53/480', '</simul>', 'rest_12', 'noteB4_2/5', 'noteC#6_53/480', 'noteE1_3/5', 'noteG7_51/1024', 'noteG#6_1/32', 'noteC6_307/512', 'noteC2_3/2', 'noteF#4_3/4', 'noteA4_215/1024', 'noteD5_1/16', 'noteG5_39/512', 'noteC#6_11/120', 'noteE3_2/5', 'noteD7_683/1024', 'noteG3_6', 'noteF#7_1/8', 'noteG#2_3/8', 'noteD3_171/512', 'noteC#7_11/160', 'noteF6_51/512', 'noteC5_205/1024', 'noteG#5_53/480', 'noteE4_3/2', 'noteC7_23/480', 'noteC#4_1/8', 'noteC#7_3/4', 'noteC1_2', 'noteF#5_103/1024', 'noteC#3_3/4', 'noteE4_103/1024', 'noteB-3_1/2', 'noteC3_6', 'noteG2_7/120', 'noteB-5_341/512', 'noteG6_1/2', 'noteF#6_4', 'noteB-3_1/10', 'noteC3_195/1024', 'noteG6_0', 'noteB2_1/10', 'noteE3_1/3', 'noteC3_1/2', 'noteE3_1/10', 'noteD2_73/1024', 'noteD5_27/128', 'noteB5_1/16', 'noteB2_1/6', 'rest_1229/512', 'noteE7_3/2', 'noteC#5_2/3', 'noteE2_1/2', 'rest_17/480', 'noteF#4_3/16', 'noteC#7_1', 'noteE4_1/16', 'rest_51/80', 'noteE-3_53/480', 'noteC3_6', 'noteA5_7/8', 'noteF#2_17/160', 'noteB-6_683/1024', '</simul>', 'noteE1_1/4', 'noteF5_3', 'noteE-6_2/5', 'noteD5_1/16', 'noteB-5_7/32', 'noteG1_1/5', 'rest_7/256', 'noteE-3_35/512', 'noteE-5_1/6', 'noteE-2_7/120', 'noteE-5_3/5', 'noteG5_43/512', 'noteC#5_3/16', 'noteB6_1/5', '</simul>', 'noteD5_1/8', 'noteA3_7/4', 'noteA6_11/160', 'noteG#5_9/256', 'noteE-6_4', 'noteB-3_195/1024', 'noteE4_3/2', 'rest_2/15', 'noteC4_3/10', 'noteF5_7/120', 'noteB-6_1/12', 'noteC7_11/96', 'rest_71/80', 'rest_5/4', 'noteE-4_15/512', 'noteE3_137/480', 'rest_7/20', 'noteF#6_3/16', 'rest_1/20', 'noteE2_1', 'rest_3', 'noteG#6_1/5', 'noteG#3_1/2', 'noteF#4_3/2', 'noteG5_3/16', 'noteF#2_341/1024', 'noteG3_37/240', 'noteC5_195/1024', 'rest_37/8', 'noteE-6_3/8', 'noteE-4_15/512', 'noteF2_3', 'rest_7/2', 'noteE-3_49/512', 'noteD6_171/512', 'noteC2_1/2', 'noteE5_1/15', 'rest_457/480', 'noteG6_3/10', 'noteD5_73/1024', 'noteC#5_29/160', 'noteE-7_73/1024', '</simul>', 'noteE3_11/240', 'noteE2_23/480', 'noteG4_1', 'noteC4_227/1024', 'noteB-6_17/160', 'rest_103/480', 'noteF#5_2', 'noteF#5_85/1024', 'rest_103/240', 'noteB-1_341/1024', 'noteG#3_83/480', 'noteF#2_51/512', 'noteA5_85/512', 'noteF5_7/96', 'noteB3_17/240', '</simul>', 'noteC6_3', 'noteG4_137/480', 'noteE-2_1/3', 'rest_1/16', 'noteE-3_341/512', 'noteE-5_53/480', 'noteE-5_7/120', 'noteD3_1/6', 'noteD5_7/8', 'noteB0_1/4', 'noteA4_19/128', '</simul>', 'noteF#5_171/1024', 'noteG#3_7/4', 'noteA3_57/256', 'noteC2_1/12', 'noteG#2_3', 'noteB5_615/1024', 'noteE5_1', 'noteG3_3/10', 'noteB-5_1/12', 'noteE-3_6', 'rest_1/10', 'noteG#6_1/16', 'rest_7/8', 'noteB2_1/2', 'noteC#5_1/4', 'noteB-4_341/1024', 'noteE2_1/12', 'noteD5_7/120', 'noteC#5_373/1024', 'noteD6_17/160', 'noteA4_1/12', 'noteF#1_0', 'noteA4_85/1024', 'noteG5_6', 'noteC#6_23/160', 'noteG5_57/512', 'noteD2_9/160', 'noteB-2_1/6', 'noteA4_69/1024', 'noteD4_195/1024', 'noteF6_2/3', 'noteE1_41/512', 'noteE2_683/1024', 'rest_1/240', 'noteC6_73/1024', 'noteB5_73/1024', 'noteB4_23/160', 'noteG5_1/15', 'noteE7_3/4', 'noteC#6_57/512', 'noteB-1_1/3', 'noteB3_4', 'rest_43/240', 'noteB6_3/2', 'rest_939/1024', 'noteG#1_6', 'noteF6_23/160', 'rest_3/2', 'noteF5_7/4', 'rest_29/240', 'noteC7_17/160', 'noteF#4_27/256', 'noteD3_1/6', 'noteG2_1/10', 'rest_15/16', 'noteF#4_79/1024', 'noteD5_37/240', 'noteE2_1/5', 'noteD2_1/16', 'noteE-4_7/512', 'noteF6_9/160', 'noteG1_2/3', 'noteG4_27/128', 'noteC#5_171/512', 'noteD5_1/8', 'noteF#1_2', 'rest_15/16', 'noteE5_17/240', 'noteD3_151/1024', 'noteE-5_1/3', 'noteF#5_1/15', 'noteE4_3/10', 'noteC4_17/160', 'noteG4_53/480', 'noteA6_4', 'noteG1_1/16', 'noteC7_3/4', 'rest_5/24', 'noteB-3_83/480', 'noteC#2_1/10', 'noteC5_1/3', 'rest_2163/1024', 'noteG4_29/160', 'noteG6_103/1024', 'noteG#5_1/32', '</simul>', 'noteB1_1/6', 'noteG5_7/96', 'noteD3_1/24', 'noteB4_205/1024', 'noteF6_3/4', 'noteB1_3', 'noteC3_39/512', 'noteG#5_3/80', 'noteB2_3', 'noteF4_1/3', 'noteE6_341/1024', 'noteF4_1/4', 'noteC#5_37/480', 'noteC4_49/256', 'noteD1_3/4', 'noteF5_7/96', 'noteD4_205/512', 'noteB5_4', 'noteC2_23/480', 'noteG#4_4', 'noteC2_1/16', 'noteG6_1', 'noteC4_53/480', 'noteE-3_107/480', 'noteE-6_1/4', 'noteF5_13/96', 'noteB4_6', 'noteC6_171/512', 'noteE6_11/160', 'noteC#2_2/3', 'noteE-3_17/240', 'noteC4_23/160', 'noteD3_151/1024', 'noteB-4_1/2', 'noteC4_7/120', 'noteG3_107/480', 'noteG5_7/96', 'noteC2_79/1024', 'noteB-3_1/2', 'noteC6_3/4', 'noteB-5_205/1024', 'noteD5_29/160', 'noteE-4_79/1024', 'noteE7_9/160', 'noteA6_1/12', 'noteC4_151/1024', 'noteF#3_1/2', 'noteG#5_3/10', 'noteG#7_1/2', 'noteB-4_171/512', 'noteC6_0', 'noteF#3_7/4', 'noteG#2_7/8', 'noteB5_17/160', 'noteB-6_3/8', 'noteB-3_19/256', 'noteG2_0', 'noteD5_215/1024', '</simul>', '<simul>', 'noteE2_23/480', 'noteE-5_37/1024', 'noteC4_17/240', 'noteE-6_1/4', 'noteE4_23/160', 'noteG2_9/8', '</simul>', 'noteF2_205/1024', 'noteC#4_7/8', 'noteG4_37/240', 'noteG6_3/2', 'noteG1_1/6', 'noteC5_13/96', 'noteE2_23/160', 'noteG4_17/240', 'noteA6_13/256', 'noteG#1_53/480', 'noteG#5_2/3', 'noteB-5_73/1024', 'noteB4_1/12', 'noteG#1_1', 'noteF2_3/8', '<simul>', 'rest_227/480', 'noteB-3_4', 'noteG5_215/1024', 'noteA4_1', 'noteC7_1/6', 'noteG#3_1/16', 'noteG#3_1/5', 'noteB-5_103/480', 'noteF#3_7/120', 'noteC5_341/512', 'noteC2_39/512', 'noteF#4_3/2', 'rest_14', 'noteB-2_107/480', 'noteC#1_1/6', 'noteD3_9/8', 'noteC3_2/3', 'noteC2_3', 'noteA1_3/4', 'noteG5_1/5', 'noteB4_3/16', 'noteC#6_85/1024', 'noteC#3_43/512', 'noteA4_215/1024', 'noteB-5_3/16', 'noteF#3_227/1024', 'noteC#7_1/16', 'noteE-6_7/4', 'noteF#5_85/512', 'noteG#2_9/160', 'noteB-2_1/5', 'noteF4_85/512', 'noteG1_1/5', 'noteA6_73/1024', 'noteG#3_17/96', 'noteE2_1', 'noteD8_1/8', 'noteB-2_7/4', 'noteF5_1/32', 'noteF#3_85/512', 'rest_65/16', 'noteE-5_1/6', 'noteF#1_1/3', 'noteG7_1/2', 'noteF4_0', 'rest_2163/1024', 'noteF#5_9/256', 'noteB6_1/32', 'noteF5_53/480', 'noteG2_9/8', 'noteE4_37/512', 'noteD4_8', 'noteB-1_17/240', 'noteE-7_53/480', 'noteB-4_3/4', 'rest_95/32', 'noteG#4_7/8', 'noteG6_17/240', 'noteC5_19/128', 'noteE-5_3/4', 'noteE-3_1/10', 'noteF#3_97/1024', 'noteB5_3/2', 'noteE-7_23/160', 'noteG#3_51/256', 'noteE7_73/1024', 'noteG3_11/120', 'noteC#1_0', 'noteC5_73/1024', 'noteB-3_37/240', 'noteB-4_13/96', 'noteC#7_1/3', 'noteF4_57/512', 'noteF#4_85/1024', 'noteF#4_37/480', 'noteG#7_1/8', 'noteE-3_51/512', 'noteE-6_3/10', 'noteG#6_43/1024', 'rest_1/3', 'noteB-7_1/3', 'noteC#7_3', 'noteF#4_1/10', 'noteC2_1/24', 'noteA3_3/4', 'noteG6_341/1024', 'noteC7_1/2', 'rest_39/4', 'noteE3_37/480', 'noteC#1_3/5', 'noteE5_2/3', 'noteC5_13/96', 'noteC#1_3/5', 'noteE5_23/480', '<simul>', 'noteG#1_53/480', 'noteB6_3', 'noteB-5_307/512', 'noteC#6_79/1024', 'noteE-3_57/512', 'noteB-3_171/1024', 'noteG#2_53/480', 'noteB-3_107/480', 'noteG3_17/96', 'noteG#4_3/32', 'noteF5_3', 'noteD6_6', 'noteA3_77/256', 'noteG#7_51/1024', 'noteG#5_1/15', 'noteE-4_1/2', 'noteG#7_1', 'noteF#6_0', 'noteE-5_3/28', 'noteE4_13/256', 'noteA6_11/160', 'rest_2133/1024', 'noteE2_37/240', 'noteG#1_1', 'noteC#6_51/512', 'noteG#2_7/8', 'noteB-4_-71/512', 'noteF#4_85/512', 'noteG4_27/128', '</simul>', 'noteA3_1/2', 'noteF#4_85/1024', 'noteB-5_13/256', 'noteG3_2', 'noteC6_0', 'noteG#7_1/10', 'noteE6_9/160', 'noteA3_1/2', 'rest_95/32', 'noteA2_6', 'noteB-2_2', 'noteD4_1/15', 'noteC#3_17/160', 'noteG2_1/4', 'noteF3_17/96', 'noteC2_39/512', 'noteB-2_2', 'noteG6_1/10', 'noteB-3_3/4', 'noteG5_79/1024', 'rest_29/96', 'noteB3_1/2', 'noteG#5_19/120', 'noteA4_1/15', 'noteB4_17/160', 'noteE-7_3/4', 'rest_5/4', 'noteC6_11/120', 'rest_253/240', 'noteC#6_37/240', 'noteE4_17/240', 'noteE-5_3/10', 'noteB-3_3/16', 'noteB4_7/4', 'noteC#3_3/16', 'noteA4_79/1024', 'noteD3_3', 'noteF6_3/2', 'noteF#2_1/8', 'noteA4_1/12', 'noteG#5_73/1024', 'noteF4_683/1024', 'noteA1_1/8', 'noteF4_3/16', 'noteA5_1', 'noteE4_6', 'noteF4_85/1024', 'noteG#4_1/16', 'noteC#4_1/16', 'noteB5_33/512', 'rest_1/24', 'noteB3_1/3', 'noteE5_51/1024', 'noteB-5_683/1024', 'noteC4_97/1024', 'noteC5_171/1024', 'noteC2_107/480', 'noteB2_1/16', 'noteC#4_85/512', 'noteG7_33/512', 'noteC4_51/1024', 'noteD3_1/2', 'noteE-4_1/2', 'noteE5_2/15', 'noteC3_4', 'noteA5_53/480', 'noteC#7_2', 'noteA6_1/2', 'noteC4_107/480', 'noteE2_341/1024', 'noteF4_43/1024', 'noteF#5_51/256', 'noteC4_341/1024', 'noteC6_2/3', 'noteA2_1/2', 'noteB6_1/4', 'noteE-4_29/240', 'noteE3_1/2', '</simul>', 'noteA3_307/1024', 'noteB-3_103/1024', 'noteE-7_1', 'noteE-3_1807/480', 'noteG#6_3/5', 'noteF6_2', 'noteF#5_9/8', 'noteB-3_107/480', 'noteG6_2/5', 'noteB-4_73/1024', 'noteG5_23/480', 'noteB4_205/1024', 'noteG#3_7/120', 'noteG4_57/1024', 'noteC6_7/120', 'noteG3_2', 'noteE4_1/4', 'noteF1_3/8', 'noteG#4_1/2', 'noteG3_107/480', 'noteB4_1/12', 'noteC#5_1/24', 'noteE3_1/2', 'noteF5_107/480', 'rest_9/160', 'noteB-4_683/1024', 'noteF1_3/8', 'noteF3_1/10', 'noteB-4_51/512', 'noteD5_73/1024', 'rest_2/15', 'noteD3_1/4', 'noteD7_1/6', 'noteE-2_57/512', 'noteG#3_171/1024', 'noteG#5_1/8', '</simul>', '<simul>', 'noteF5_7/4', 'noteF4_171/512', 'noteC5_7/120', 'noteE3_73/1024', 'noteF#4_3/8', 'noteC7_1/12', 'noteG4_39/512', '</simul>', 'noteG#1_1/12', 'noteB-4_1/4', 'noteD6_171/1024', 'noteC4_227/1024', 'noteF#6_2/3', 'noteA5_6', 'noteG5_73/1024', 'noteG4_7/512', 'noteC5_103/480', 'rest_1/48', 'noteD4_0', '</simul>', 'noteG#2_1/16', 'noteF5_1/10', 'noteB-5_17/256', 'noteF2_3/2', 'rest_39/8', 'noteF7_1/6', 'noteE-4_1/4', 'noteC4_7/120', 'noteD6_53/480', 'rest_7/3', 'noteD6_11/128', 'noteE5_3/10', 'noteD5_5/96', 'noteF#2_3/8', 'noteC1_4', 'noteC#2_1/12', 'noteC5_1/3', 'rest_113/480', 'noteC#2_23/160', 'noteF#5_2/3', 'rest_47/8', 'noteE3_1/3', 'noteF#6_341/1024', 'noteG#1_683/1024', 'noteB6_43/512', 'noteC#7_0', 'noteF#4_3/2', 'noteA2_1/15', 'noteB5_1/2', 'noteE4_5/96', 'noteF#2_107/480', 'noteE-6_3/8', 'noteF#6_315/1024', 'rest_67/240', 'noteB-0_1/3', 'noteE3_1', 'noteA3_57/512', 'noteG#5_21/512', 'noteG6_341/1024', 'rest_67/240', 'noteC6_9/10', 'noteG6_3/10', 'noteC#3_103/1024', 'noteG5_17/240', 'noteF#7_13/256', 'noteE3_6', 'noteC#3_1/8', 'noteB-4_341/512', 'noteE-5_3/2', 'noteC6_33/512', 'noteB-4_107/480', 'noteE3_7/4', 'noteB-5_23/160', 'noteE5_3/16', 'noteC#5_103/480', 'noteA2_341/512', '</simul>', 'rest_57/1024', 'noteG#5_3/16', 'noteB-6_3/4', 'noteG5_1/2', 'noteD7_1/2', 'noteC3_1/16', 'noteA3_1/3', 'noteE-2_17/160', 'noteA5_341/1024', 'noteG#7_73/1024', 'noteE-3_49/512', 'noteE-1_3/2', 'noteG#5_37/480', 'noteE6_37/480', 'noteB-6_43/512', 'rest_95/32', 'noteF#6_3/4', 'noteE4_7/4', 'noteC#3_3/10', 'noteB5_1/5', '</simul>', 'noteE4_51/512', 'noteB2_1/2', 'noteC2_107/480', 'noteG6_1/15', 'noteA3_11/96', 'noteC#3_3/2', 'rest_1/128', 'noteB4_1/2', 'rest_3/2', 'noteG#2_7/8', 'noteE2_0', 'noteB4_171/512', 'noteF3_2', 'noteG5_1/4', 'noteE2_23/480', 'noteG3_53/480', '<simul>', 'noteG#1_3/16', 'noteG6_1/12', 'noteG#6_1/2', 'noteE3_57/512', 'noteE5_1/12', '</simul>', 'noteC3_6', 'noteF4_3/2', 'noteG#2_4', 'noteE-3_11/120', 'noteB6_683/1024', 'noteE-3_1807/480', 'noteB3_113/1024', 'noteG6_3/4', 'noteF#7_43/1024', 'noteD7_2', 'noteC#5_2', 'noteA1_3/4', 'noteG2_57/1024', 'noteF#3_85/512', 'noteC7_1/10', 'noteC#7_3/8', 'noteE-1_1', 'noteC#5_27/256', 'rest_1/30', 'noteG#6_1/32', 'noteD3_113/1024', 'noteF#4_27/256', 'noteB-3_3/2', 'noteE-6_3/8', 'noteF5_7/8', 'noteE5_3', '</simul>', 'noteF4_113/1024', 'noteG4_6', 'noteG#5_6/7', 'noteD6_29/160', 'noteG6_1/2', 'noteB-4_2', 'noteG5_1/3', 'noteC5_1/20', 'noteB-5_1/15', 'noteE6_85/512', 'noteD3_2/3', 'noteG#4_1', 'noteE-2_1', 'noteE6_1/4', 'noteA4_113/1024', 'noteB4_683/1024', 'noteB-5_3/8', 'noteC#5_113/1024', 'noteC6_1/16', 'noteB5_37/480', 'noteF4_53/480', 'noteB-4_3', 'noteF#2_53/480', 'noteD5_195/1024', 'noteE5_11/96', 'noteD3_1/24', 'noteF2_7/4', 'noteA4_69/1024', 'noteF#3_19/128', 'noteE6_2', 'noteD4_3/4', 'noteG4_17/240', 'noteE-7_341/512', '</simul>', 'rest_97/16', '</simul>', 'noteB1_3/4', 'noteC4_103/1024', 'noteG4_2', 'noteA3_341/1024', 'noteG#5_21/512', 'noteA6_33/512', 'noteE-2_1/8', 'noteC#4_227/1024', 'noteF#1_107/480', 'noteE7_17/240', 'noteA6_3/4', 'noteC#2_41/512', 'noteA1_0', 'noteG#5_6', 'noteD3_3/5', 'noteG#1_171/1024', 'noteC#4_3/4', 'noteE2_1/2', 'noteE-5_617/240', 'noteG5_1/3', 'noteC#6_37/240', 'noteE2_1/3', 'noteG7_1/2', 'noteG3_3/4', 'noteC5_37/240', 'noteA4_29/240', 'noteE4_195/1024', 'noteB1_1/4', 'noteF2_3/4', 'noteB5_617/240', 'noteD1_41/512', 'noteB4_1/2', 'noteF#5_1/3', 'noteE3_683/1024', 'noteE6_9/160', 'rest_1/480', 'noteB-4_1/4', 'noteB-3_1/3', 'noteF2_1/12', 'noteE-7_1/12', 'noteC5_1/2', 'noteF#6_7/256', 'noteC6_23/480', 'noteE3_17/240', 'noteE-4_7/160', 'noteE3_683/1024', 'noteF#1_27/256', 'noteE6_51/1024', 'noteF1_6', 'noteE-4_3/10', '</simul>', '</simul>', 'noteE-4_107/480', 'noteC#2_2', 'noteB-3_73/1024', 'noteB3_79/1024', 'noteB-2_23/160', 'noteE4_57/512', 'noteB1_107/480', 'noteB3_3/2', 'noteG#5_79/1024', 'noteG#3_3/2', 'rest_131/160', 'noteB0_2', 'noteE-4_2', 'noteF#6_1', 'noteF5_11/128', 'noteE-3_2/3', 'noteC#6_1/6', 'noteC5_107/480', 'noteC4_103/1024', 'noteC#6_2', 'noteB-6_3', 'noteB-5_151/1024', 'noteF3_341/512', 'noteF#7_79/1024', 'noteD5_1/8', 'noteF6_7/4', '<simul>', 'noteA3_53/480', 'noteE-6_0', 'noteF6_2/3', 'noteF3_57/512', 'noteD2_17/160', 'noteC#6_3', 'noteA4_1/32', 'noteF#6_17/256', 'noteE-3_1/16', 'noteG5_2/3', 'noteG#3_51/256', 'noteC5_1/3', 'noteD7_683/1024', 'noteE4_1/5', 'noteG5_107/480', 'noteF4_4', 'noteE7_23/160', 'noteG5_7/8', 'noteC3_7/8', 'noteF5_1/4', 'noteG#4_103/1024', 'noteE2_195/1024', 'noteF2_4', 'noteB4_65/1024', 'noteC#3_23/160', 'noteC#6_35/96', '</simul>', 'noteA7_1', 'noteG4_3', 'noteD2_341/1024', 'rest_33/2']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# this just grabs a random batch and performs on it\n",
    "for batch in validation_dataloader:\n",
    "    bleh = generate_music(model, batch[0][:4], temperature=1.0)\n",
    "    notes = [token_to_note[token] for token in bleh]\n",
    "    print(print(notes))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"music_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
